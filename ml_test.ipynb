{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hatim\\AppData\\Local\\Temp\\ipykernel_12756\\1744325731.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "C:\\Users\\Hatim\\AppData\\Local\\Temp\\ipykernel_12756\\1744325731.py:70: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('enron_05_17_2015_with_labels_v2.csv\\enron_05_17_2015_with_labels_v2.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                     Message-ID  \\\n",
      "0           0  <18782981.1075855378110.JavaMail.evans@thyme>   \n",
      "1           1  <15464986.1075855378456.JavaMail.evans@thyme>   \n",
      "2           2  <24216240.1075855687451.JavaMail.evans@thyme>   \n",
      "3           3  <13505866.1075863688222.JavaMail.evans@thyme>   \n",
      "4           4  <30922949.1075863688243.JavaMail.evans@thyme>   \n",
      "\n",
      "                  Date                                    From  \\\n",
      "0  2001-05-14 23:39:00  frozenset({'phillip.allen@enron.com'})   \n",
      "1  2001-05-04 20:51:00  frozenset({'phillip.allen@enron.com'})   \n",
      "2  2000-10-18 10:00:00  frozenset({'phillip.allen@enron.com'})   \n",
      "3  2000-10-23 13:13:00  frozenset({'phillip.allen@enron.com'})   \n",
      "4  2000-08-31 12:07:00  frozenset({'phillip.allen@enron.com'})   \n",
      "\n",
      "                                       To    Subject           X-From  \\\n",
      "0     frozenset({'tim.belden@enron.com'})        NaN  Phillip K Allen   \n",
      "1  frozenset({'john.lavorato@enron.com'})        Re:  Phillip K Allen   \n",
      "2   frozenset({'leah.arsdall@enron.com'})   Re: test  Phillip K Allen   \n",
      "3    frozenset({'randall.gay@enron.com'})        NaN  Phillip K Allen   \n",
      "4     frozenset({'greg.piper@enron.com'})  Re: Hello  Phillip K Allen   \n",
      "\n",
      "                                                X-To X-cc X-bcc  ...  \\\n",
      "0           Tim Belden <Tim Belden/Enron@EnronXGate>  NaN   NaN  ...   \n",
      "1  John J Lavorato <John J Lavorato/ENRON@enronXg...  NaN   NaN  ...   \n",
      "2                                   Leah Van Arsdall  NaN   NaN  ...   \n",
      "3                                      Randall L Gay  NaN   NaN  ...   \n",
      "4                                         Greg Piper  NaN   NaN  ...   \n",
      "\n",
      "  Cat_10_level_1 Cat_10_level_2 Cat_10_weight Cat_11_level_1 Cat_11_level_2  \\\n",
      "0            NaN            NaN           NaN            NaN            NaN   \n",
      "1            NaN            NaN           NaN            NaN            NaN   \n",
      "2            NaN            NaN           NaN            NaN            NaN   \n",
      "3            NaN            NaN           NaN            NaN            NaN   \n",
      "4            NaN            NaN           NaN            NaN            NaN   \n",
      "\n",
      "   Cat_11_weight  Cat_12_level_1  Cat_12_level_2  Cat_12_weight  labeled  \n",
      "0            NaN             NaN             NaN            NaN    False  \n",
      "1            NaN             NaN             NaN            NaN    False  \n",
      "2            NaN             NaN             NaN            NaN    False  \n",
      "3            NaN             NaN             NaN            NaN    False  \n",
      "4            NaN             NaN             NaN            NaN    False  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Conv1D, MaxPooling1D, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Check if tensorflow.keras.wrappers is installed\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Function to predict spam/ham category\n",
    "def predict_spam_ham(text, model, tokenizer, max_len, label_encoder):\n",
    "    cleaned_text = clean_text(text)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    return label_encoder.inverse_transform(np.argmax(prediction, axis=1))[0]\n",
    "\n",
    "# Function to plot training metrics\n",
    "def plot_training_metrics(history, img_prefix='training'):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{img_prefix}_accuracy.png')  # Save as image\n",
    "    plt.show()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{img_prefix}_loss.png')  # Save as image\n",
    "    plt.show()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('enron_05_17_2015_with_labels_v2.csv\\enron_05_17_2015_with_labels_v2.csv')\n",
    "\n",
    "# Display initial data information\n",
    "print(df.head())\n",
    "\n",
    "# Combine and clean text data\n",
    "df['Text'] = df['Subject'].fillna('') + ' ' + df['content'].fillna('')\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label'] = label_encoder.fit_transform(df['labeled'])\n",
    "\n",
    "# Tokenization and Padding\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Text'])\n",
    "X = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "y = df['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building the model\n",
    "def create_model(dropout_rate=0.0, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_WORDS, 128, input_length=MAX_LEN))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 50]\n",
    "epochs = [10, 50, 100]\n",
    "dropout_rate = [0.1, 0.5, 0.9]\n",
    "optimizer = ['adam', 'nadam', 'sgd']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, dropout_rate=dropout_rate, optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(f\"Best: {grid_result.best_score_:.2f} using {grid_result.best_params_}\")\n",
    "\n",
    "# Train the best model\n",
    "best_model = grid_result.best_estimator_.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = best_model.model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Plot training metrics\n",
    "plot_training_metrics(best_model.model.history, img_prefix='best_model')\n",
    "\n",
    "# Example prediction\n",
    "example_text = \"Win a brand new car! Click here for details.\"\n",
    "prediction = predict_spam_ham(example_text, best_model.model, tokenizer, MAX_LEN, label_encoder)\n",
    "print(f\"Prediction: {prediction}\")\n",
    "\n",
    "# Additional Feature Engineering\n",
    "df['text_length'] = df['content'].apply(len)\n",
    "df['subject_length'] = df['Subject'].apply(len)\n",
    "df['count_exclamation'] = df['content'].apply(lambda x: x.count('!'))\n",
    "df['count_links'] = df['content'].apply(lambda x: len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', x)))\n",
    "\n",
    "# Combine and clean text data\n",
    "df['Text'] = df['Subject'] + ' ' + df['content']\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Preparing the text data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "# Converting to DataFrame to concatenate with other features\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate TF-IDF features with engineered features\n",
    "X = pd.concat([tfidf_df, df[['text_length', 'subject_length', 'count_exclamation', 'count_links']]], axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# RFE with RandomForestClassifier\n",
    "forest = RandomForestClassifier()\n",
    "rfe = RFE(estimator=forest, n_features_to_select=10, step=1)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Transform X to the selected features\n",
    "X_transformed = rfe.transform(X)\n",
    "\n",
    "# Cross-validation score\n",
    "scores = cross_val_score(forest, X_transformed, y, cv=5)\n",
    "print(f\"Mean cross-validation score: {scores.mean():.2f}\")\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Conv1D, MaxPooling1D, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Function to predict spam/ham category\n",
    "def predict_spam_ham(text, model, tokenizer, max_len, label_encoder):\n",
    "    cleaned_text = clean_text(text)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    return label_encoder.inverse_transform(np.argmax(prediction, axis=1))[0]\n",
    "\n",
    "# Function to plot training metrics\n",
    "def plot_training_metrics(history, img_prefix='training'):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{img_prefix}_accuracy.png')  # Save as image\n",
    "    plt.show()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{img_prefix}_loss.png')  # Save as image\n",
    "    plt.show()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('enron_05_17_2015_with_labels_v2.csv\\enron_05_17_2015_with_labels_v2.csv')\n",
    "\n",
    "# Display initial data information\n",
    "print(df.head())\n",
    "\n",
    "# Combine and clean text data\n",
    "df['Text'] = df['Subject'].fillna('') + ' ' + df['content'].fillna('')\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label'] = label_encoder.fit_transform(df['labeled'])\n",
    "\n",
    "# Tokenization and Padding\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Text'])\n",
    "X = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "y = df['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building the model\n",
    "def create_model(dropout_rate=0.0, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_WORDS, 128, input_length=MAX_LEN))\n",
    "    model.add(Conv1D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters with model__ prefix\n",
    "param_grid = {\n",
    "    'model__dropout_rate': [0.1, 0.5, 0.9],\n",
    "    'model__optimizer': ['adam', 'nadam', 'sgd'],\n",
    "    'batch_size': [10, 20, 50],\n",
    "    'epochs': [10, 50, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(f\"Best: {grid_result.best_score_:.2f} using {grid_result.best_params_}\")\n",
    "\n",
    "# Train the best model\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = best_model.model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Plot training metrics\n",
    "plot_training_metrics(best_model.model.history, img_prefix='best_model')\n",
    "\n",
    "# Example prediction\n",
    "example_text = \"Win a brand new car! Click here for details.\"\n",
    "prediction = predict_spam_ham(example_text, best_model.model, tokenizer, MAX_LEN, label_encoder)\n",
    "print(f\"Prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Additional Feature Engineering\n",
    "df['text_length'] = df['content'].apply(len)\n",
    "df['subject_length'] = df['Subject'].apply(len)\n",
    "df['count_exclamation'] = df['content'].apply(lambda x: x.count('!'))\n",
    "df['count_links'] = df['content'].apply(lambda x: len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', x)))\n",
    "\n",
    "# Combine and clean text data\n",
    "df['Text'] = df['Subject'] + ' ' + df['content']\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Preparing the text data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "# Converting to DataFrame to concatenate with other features\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate TF-IDF features with engineered features\n",
    "X = pd.concat([tfidf_df, df[['text_length', 'subject_length', 'count_exclamation', 'count_links']]], axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# RFE with RandomForestClassifier\n",
    "forest = RandomForestClassifier()\n",
    "rfe = RFE(estimator=forest, n_features_to_select=10, step=1)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Transform X to the selected features\n",
    "X_transformed = rfe.transform(X)\n",
    "\n",
    "# Cross-validation score\n",
    "scores = cross_val_score(forest, X_transformed, y, cv=5)\n",
    "print(f\"Mean cross-validation score: {scores.mean():.2f}\")\n",
    "\n",
    "# Error analysis with confusion matrix\n",
    "y_pred = best_model.model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, np.argmax(y_pred, axis=1))\n",
    "report = classification_report(y_test, np.argmax(y_pred, axis=1), target_names=label_encoder.classes_)\n",
    "print(report)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[-10:]  # top 10 features\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [tfidf_vectorizer.get_feature_names_out()[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# Feature Imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SHAP values for model explainability\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(forest)\n",
    "shap_values = explainer.shap_values(X_transformed)\n",
    "\n",
    "shap.summary_plot(shap_values, X_transformed, feature_names=[tfidf_vectorizer.get_feature_names_out()[i] for i in indices])\n",
    "\n",
    "# Save the model\n",
    "best_model.model.save('best_model.h5')\n",
    "\n",
    "# Documenting the code with comments and docstrings\n",
    "# Ensure your code is well-documented with comments explaining the purpose of each section and docstrings for functions\n",
    "\n",
    "# Error analysis with confusion matrix\n",
    "y_pred = best_model.model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, np.argmax(y_pred, axis=1))\n",
    "report = classification_report(y_test, np.argmax(y_pred, axis=1), target_names=label_encoder.classes_)\n",
    "print(report)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
